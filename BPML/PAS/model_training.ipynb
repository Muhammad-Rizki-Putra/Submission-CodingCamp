{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3fa4b4b",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "c19888a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import csv\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import swifter\n",
    "import requests\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from keras.layers import MaxPooling1D\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.utils import resample\n",
    "from keras.layers import GRU\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, Bidirectional, LSTM,\n",
    "    Conv1D, GlobalMaxPooling1D,\n",
    "    Dense, Dropout\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd77f1a",
   "metadata": {},
   "source": [
    "# Data Loading & Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9f3756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_reviews_df = pd.read_csv(\"hasil_scrapping.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bde7d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "b9da1c40-1ae7-41ab-9e01-4db533964a12",
       "rows": [
        [
         "0",
         "terlalu terlalu terlalu... apk yg tidak bisa di percaya. kuota cepat abis minta update terus. tp bug tmbah banyak. hp jd lelet !!!.. kok bisa nyuri data pribadi ya... diliat dari apk yg terus berjln dilatar belakang dan terus mengirim data. tp g tau data ap yg dikirim ?? wah wah wah... bahaya bobol bisa atm wkwkwk"
        ],
        [
         "1",
         "Gak usah pasang tarif tarif hemat, soalnya para driver nya gak ada yang mau ambil orderan, dengan alasan gak sesuai dengan harga, dan bahasa mereka merendahkan costumer, karena pesen yg paket hemat, trs knp di adain paket hemat, yg salah yg punya aplikasi lah, pengen kasih promo tapi para driver nya gak mau."
        ],
        [
         "2",
         "tinggal 2menit lg driver sampe di lokasi tiba-tiba dibatalin otomatis sama aplikasinya!!! tau tau udah mencari driver yg baru, dan ini kejadian gak sekali dua kali tapi sering! map jg sering di arahin ke jalan yg sulit di lewati mobil!! alhasil banyak yg batalin! komplain via chat cuma bot doang, Kita yg mo complain gak bisa ketik sendiri masalahnya, mo complain lewat email atau tlp jg gak bisa!!! aplikasi gak niat terima komplain!!!"
        ],
        [
         "3",
         "sebagai pengguna lama baru kali ini saya kecewa sama aplikasi GO-JEK terutama gocar... sebelumnya saya tidak pernah ada masalah walau kadang tidak sesuai harapan dan tetap saya kasih bintang lima dan tip. tp ini sudah pakai prioritas, tetap dicancel driver dgn alasan jauh (pdhl di maps deket), harga ga sesuai (kan bukan saya yg kasih harga jg), driver yg beneran jauh posisinya (pdhl prioritas harusnya yg paling dekat)... akhirnya adu lama2an cancel order... makan waktu... makan ati... hiks..."
        ],
        [
         "4",
         "susah untuk dpt driver walaupun di map ada banyak driver yg gak jalan, sekalinya hujan harga langsung melonjak drastis setiap merefresh, ada pilihan hemat namun harga lebih mahal daripada yg biasa dan percuma tidak akan ada driver yg ambil orderannya,sudah memilih dari yg murah sampai paling mahal namun tetap saja nihilğŸ¥´, dulu enak sekarang makin buruk"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>terlalu terlalu terlalu... apk yg tidak bisa d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gak usah pasang tarif tarif hemat, soalnya par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tinggal 2menit lg driver sampe di lokasi tiba-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sebagai pengguna lama baru kali ini saya kecew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>susah untuk dpt driver walaupun di map ada ban...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content\n",
       "0  terlalu terlalu terlalu... apk yg tidak bisa d...\n",
       "1  Gak usah pasang tarif tarif hemat, soalnya par...\n",
       "2  tinggal 2menit lg driver sampe di lokasi tiba-...\n",
       "3  sebagai pengguna lama baru kali ini saya kecew...\n",
       "4  susah untuk dpt driver walaupun di map ada ban..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_reviews_df = app_reviews_df[['content']]\n",
    "app_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2329631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisis_data(df):\n",
    "    print(df.info())\n",
    "    print(\"\\n-------------------------------------------------------------------------------\\n\")\n",
    "    print(f\"Data Kosong : \\n\\n{df.isna().sum()}\")\n",
    "    print(\"\\n-------------------------------------------------------------------------------\\n\")\n",
    "    print(f\"Data ganda  : {df.duplicated().sum()}\")\n",
    "    print(\"\\n-------------------------------------------------------------------------------\\n\")\n",
    "    print(df.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a6f85b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 1 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   content  50000 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 390.8+ KB\n",
      "None\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Data Kosong : \n",
      "\n",
      "content    0\n",
      "dtype: int64\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Data ganda  : 165\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "<bound method NDFrame.describe of                                                  content\n",
      "0      terlalu terlalu terlalu... apk yg tidak bisa d...\n",
      "1      Gak usah pasang tarif tarif hemat, soalnya par...\n",
      "2      tinggal 2menit lg driver sampe di lokasi tiba-...\n",
      "3      sebagai pengguna lama baru kali ini saya kecew...\n",
      "4      susah untuk dpt driver walaupun di map ada ban...\n",
      "...                                                  ...\n",
      "49995  kenpa si setiap saya order goride dapetnya dri...\n",
      "49996  Dari dulu sampe sekarang sama aja untuk masala...\n",
      "49997  Pinjaman di playleter telalu banyak tambahan s...\n",
      "49998  Sudah bagus sekali sih, hanya satu/2an lah kur...\n",
      "49999  Lama lama kok jadi gampang nge freeze sih apli...\n",
      "\n",
      "[50000 rows x 1 columns]>\n"
     ]
    }
   ],
   "source": [
    "analisis_data(app_reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "524b39c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 49835 entries, 0 to 49999\n",
      "Data columns (total 1 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   content  49835 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 778.7+ KB\n",
      "None\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Data Kosong : \n",
      "\n",
      "content    0\n",
      "dtype: int64\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Data ganda  : 0\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "<bound method NDFrame.describe of                                                  content\n",
      "0      terlalu terlalu terlalu... apk yg tidak bisa d...\n",
      "1      Gak usah pasang tarif tarif hemat, soalnya par...\n",
      "2      tinggal 2menit lg driver sampe di lokasi tiba-...\n",
      "3      sebagai pengguna lama baru kali ini saya kecew...\n",
      "4      susah untuk dpt driver walaupun di map ada ban...\n",
      "...                                                  ...\n",
      "49995  kenpa si setiap saya order goride dapetnya dri...\n",
      "49996  Dari dulu sampe sekarang sama aja untuk masala...\n",
      "49997  Pinjaman di playleter telalu banyak tambahan s...\n",
      "49998  Sudah bagus sekali sih, hanya satu/2an lah kur...\n",
      "49999  Lama lama kok jadi gampang nge freeze sih apli...\n",
      "\n",
      "[49835 rows x 1 columns]>\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = app_reviews_df.drop_duplicates()\n",
    "analisis_data(cleaned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e4037e",
   "metadata": {},
   "source": [
    "# Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "635e4c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) \n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text) \n",
    "    text = re.sub(r'RT[\\s]', '', text) \n",
    "    text = re.sub(r\"http\\S+\", '', text) \n",
    "    text = re.sub(r'[0-9]+', '', text) \n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    " \n",
    "    text = text.replace('\\n', ' ') \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) \n",
    "    text = text.strip(' ') \n",
    "    return text\n",
    " \n",
    "def casefoldingText(text): \n",
    "    text = text.lower()\n",
    "    return text\n",
    " \n",
    "def tokenizingText(text): \n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    " \n",
    "def filteringText(text): \n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    listStopwords1 = set(stopwords.words('english'))\n",
    "    listStopwords.update(listStopwords1)\n",
    "\n",
    "    keep_words = {\"baik\", \"buruk\", \"jelek\", \"bagus\", \"senang\", \"marah\", \"puas\", \"kecewa\"}\n",
    "    listStopwords = listStopwords - keep_words\n",
    "\n",
    "    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku',\"di\",\"ga\",\"ya\",\"gaa\",\"loh\",\"kah\",\"woi\",\"woii\",\"woy\"])\n",
    "    \n",
    "    filtered = [txt for txt in text if txt not in listStopwords]\n",
    "    return filtered\n",
    "\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "def stemmingText(text): \n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    return stemmed_words\n",
    " \n",
    "def toSentence(list_words): \n",
    "    sentence = ' '.join(word for word in list_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0dc38f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "slangwords = {}\n",
    "with open('kamusalay.csv', mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        if len(row) >= 2:\n",
    "            slang = row[0].strip().lower()\n",
    "            normal = row[1].strip().lower()\n",
    "            slangwords[slang] = normal\n",
    "\n",
    "def fix_slangwords(text):\n",
    "    words = text.split()\n",
    "    fixed_words = []\n",
    " \n",
    "    for word in words:\n",
    "        if word.lower() in slangwords:\n",
    "            fixed_words.append(slangwords[word.lower()])\n",
    "        else:\n",
    "            fixed_words.append(word)\n",
    " \n",
    "    fixed_text = ' '.join(fixed_words)\n",
    "    return fixed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc64d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10966e05b95c49a3a50dbb99d768b9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/49835 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rizki\\AppData\\Local\\Temp\\ipykernel_5140\\1059241690.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['text_clean'] = cleaned_df['content'].swifter.progress_bar(True).apply(cleaningText)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784789397a7f493ab2f75df4060ad188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/49835 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rizki\\AppData\\Local\\Temp\\ipykernel_5140\\1059241690.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['text_casefoldingText'] = cleaned_df['text_clean'].swifter.progress_bar(True).apply(casefoldingText)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3d7f7ae4f644a48fb1dd4d49193244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/49835 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rizki\\AppData\\Local\\Temp\\ipykernel_5140\\1059241690.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['text_slangwords'] = cleaned_df['text_casefoldingText'].swifter.progress_bar(True).apply(fix_slangwords)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015e3db56096420d98c5db5b3c86b9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/49835 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rizki\\AppData\\Local\\Temp\\ipykernel_5140\\1059241690.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['text_tokenizingText'] = cleaned_df['text_slangwords'].swifter.progress_bar(True).apply(tokenizingText)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe5d33aa69f435bacf9dbe05692a121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/49835 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rizki\\AppData\\Local\\Temp\\ipykernel_5140\\1059241690.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['text_stopword'] = cleaned_df['text_tokenizingText'].swifter.progress_bar(True).apply(filteringText)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c39391bdca43fe8e05f10cc30e75c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/49835 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rizki\\AppData\\Local\\Temp\\ipykernel_5140\\1059241690.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['text_stemming'] = cleaned_df['text_stopword'].swifter.progress_bar(True).apply(stemmingText)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e72a2b454c140df8367d25b518dc12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/49835 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rizki\\AppData\\Local\\Temp\\ipykernel_5140\\1059241690.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['text_akhir'] = cleaned_df['text_stemming'].swifter.progress_bar(True).apply(toSentence)\n"
     ]
    }
   ],
   "source": [
    "cleaned_df['text_clean'] = cleaned_df['content'].swifter.progress_bar(True).apply(cleaningText)\n",
    "cleaned_df['text_casefoldingText'] = cleaned_df['text_clean'].swifter.progress_bar(True).apply(casefoldingText)\n",
    "cleaned_df['text_slangwords'] = cleaned_df['text_casefoldingText'].swifter.progress_bar(True).apply(fix_slangwords)\n",
    "cleaned_df['text_tokenizingText'] = cleaned_df['text_slangwords'].swifter.progress_bar(True).apply(tokenizingText)\n",
    "cleaned_df['text_stopword'] = cleaned_df['text_tokenizingText'].swifter.progress_bar(True).apply(filteringText)\n",
    "cleaned_df['text_stemming'] = cleaned_df['text_stopword'].swifter.progress_bar(True).apply(stemmingText)\n",
    "cleaned_df['text_akhir'] = cleaned_df['text_stemming'].swifter.progress_bar(True).apply(toSentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6972b5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon Positif: 3606 kata\n",
      "Lexicon Negatif: 5463 kata\n",
      "Tidak ada duplikat antar lexicon\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# UNTUK SKIP YANG TERDAPAT DI LEXICON\n",
    "words_to_exclude = {\"aplikasi\", \"aplikasinya\", \"aja\"}\n",
    "\n",
    "def load_lexicon_from_tsv(url, skip_words=None):\n",
    "    lexicon = {}\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        tsv_content = response.text.strip().split('\\n')\n",
    "        reader = csv.reader(tsv_content, delimiter='\\t')\n",
    "        next(reader)  # skip header\n",
    "        for row in reader:\n",
    "            if len(row) >= 2:\n",
    "                word = row[0].strip().lower()\n",
    "                weight = int(row[1])\n",
    "                if not skip_words or word not in skip_words:\n",
    "                    lexicon[word] = weight\n",
    "        return lexicon\n",
    "    else:\n",
    "        raise Exception(f\"Gagal memuat lexicon dari {url}\")\n",
    "\n",
    "url_positive = \"https://raw.githubusercontent.com/fajri91/InSet/master/positive.tsv\"\n",
    "url_negative = \"https://raw.githubusercontent.com/fajri91/InSet/master/negative.tsv\"\n",
    "\n",
    "lexicon_positive = load_lexicon_from_tsv(url_positive, skip_words=words_to_exclude)\n",
    "lexicon_negative = load_lexicon_from_tsv(url_negative, skip_words=set(lexicon_positive.keys()) | words_to_exclude)\n",
    "\n",
    "print(f\"Lexicon Positif: {len(lexicon_positive)} kata\")\n",
    "print(f\"Lexicon Negatif: {len(lexicon_negative)} kata\")\n",
    "\n",
    "overlap = set(lexicon_positive.keys()) & set(lexicon_negative.keys())\n",
    "if overlap:\n",
    "    print(\"Duplikat ditemukan:\", overlap)\n",
    "else:\n",
    "    print(\"Tidak ada duplikat antar lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_lexicon_indonesia(text):\n",
    "    score = 0\n",
    "    for word in text.split():\n",
    "        if word in lexicon_positive:\n",
    "            score += lexicon_positive[word]\n",
    "        if word in lexicon_negative:\n",
    "            score += lexicon_negative[word]\n",
    "    \n",
    "    #Memperluas Neutral\n",
    "    if score > 2:\n",
    "        polarity = 'positive'\n",
    "    elif score < -1:\n",
    "        polarity = 'negative'\n",
    "    else:\n",
    "        polarity = 'neutral'\n",
    "    \n",
    "    return polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "b2547c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013c9370a97c4dcdbc9f5959406601d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/49835 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity\n",
      "positive    32075\n",
      "negative     9869\n",
      "neutral      7891\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rizki\\AppData\\Local\\Temp\\ipykernel_5140\\805663873.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['polarity'] = cleaned_df['text_akhir'].swifter.apply(sentiment_analysis_lexicon_indonesia)\n"
     ]
    }
   ],
   "source": [
    "swifter.config.display_progressbar = True\n",
    "cleaned_df['polarity'] = cleaned_df['text_akhir'].swifter.apply(sentiment_analysis_lexicon_indonesia)\n",
    "print(cleaned_df['polarity'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "d512d2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediksi Sentimen: neutral\n"
     ]
    }
   ],
   "source": [
    "text_input = \"biasa aja\"\n",
    "result = sentiment_analysis_lexicon_indonesia(text_input)\n",
    "print(\"Prediksi Sentimen:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0594dc",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c12a5d",
   "metadata": {},
   "source": [
    "### Traditional Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "a5cecbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = cleaned_df['text_akhir']\n",
    "y = cleaned_df['polarity']\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=7000, min_df=2, max_df=0.9)\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "aa82ed2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - accuracy_train: 0.9027290057188723\n",
      "Logistic Regression - accuracy_test: 0.8557238888331494\n"
     ]
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression()\n",
    " \n",
    "logistic_regression.fit(X_train.toarray(), y_train)\n",
    " \n",
    "y_pred_train_lr = logistic_regression.predict(X_train.toarray())\n",
    "y_pred_test_lr = logistic_regression.predict(X_test.toarray())\n",
    " \n",
    "accuracy_train_lr = accuracy_score(y_pred_train_lr, y_train)\n",
    " \n",
    "accuracy_test_lr = accuracy_score(y_pred_test_lr, y_test)\n",
    " \n",
    "print('Logistic Regression - accuracy_train:', accuracy_train_lr)\n",
    "print('Logistic Regression - accuracy_test:', accuracy_test_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df353cd2",
   "metadata": {},
   "source": [
    "### DeepLearning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "4e18e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "bbffc386",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 7000\n",
    "MAX_LEN = 150\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "X_pad = pad_sequences(X_seq, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "d639e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "d8ec4b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rizki\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 290ms/step - accuracy: 0.7390 - loss: 0.6545 - val_accuracy: 0.8648 - val_loss: 0.3282 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 276ms/step - accuracy: 0.9000 - loss: 0.2508 - val_accuracy: 0.8896 - val_loss: 0.2678 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 305ms/step - accuracy: 0.9233 - loss: 0.1931 - val_accuracy: 0.9043 - val_loss: 0.2430 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 277ms/step - accuracy: 0.9458 - loss: 0.1435 - val_accuracy: 0.9052 - val_loss: 0.2627 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 282ms/step - accuracy: 0.9561 - loss: 0.1176 - val_accuracy: 0.9044 - val_loss: 0.2966 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 294ms/step - accuracy: 0.9726 - loss: 0.0781 - val_accuracy: 0.9135 - val_loss: 0.2834 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential([\n",
    "    Embedding(MAX_FEATURES, 128, input_length=MAX_LEN),\n",
    "    Bidirectional(LSTM(128)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
    "\n",
    "history = model_lstm.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stop, lr_scheduler]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "d6139602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Train Accuracy: 0.953875720500946\n",
      "LSTM Test Accuracy: 0.9042873382568359\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = model_lstm.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"LSTM Train Accuracy:\", train_accuracy)\n",
    "\n",
    "test_loss, test_accuracy = model_lstm.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"LSTM Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "ea0fadec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_encoded, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "1eadee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_lstm():\n",
    "    model = Sequential([\n",
    "        Embedding(MAX_FEATURES, 128),\n",
    "        Conv1D(64, kernel_size=5, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Bidirectional(LSTM(128)),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "c1bb8044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 101ms/step - accuracy: 0.7190 - loss: 0.6869 - val_accuracy: 0.8816 - val_loss: 0.2894 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 102ms/step - accuracy: 0.9013 - loss: 0.2508 - val_accuracy: 0.8834 - val_loss: 0.2863 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 103ms/step - accuracy: 0.9343 - loss: 0.1706 - val_accuracy: 0.9019 - val_loss: 0.2530 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 106ms/step - accuracy: 0.9539 - loss: 0.1262 - val_accuracy: 0.9044 - val_loss: 0.2661 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 105ms/step - accuracy: 0.9641 - loss: 0.1028 - val_accuracy: 0.9036 - val_loss: 0.2879 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 103ms/step - accuracy: 0.9818 - loss: 0.0571 - val_accuracy: 0.9098 - val_loss: 0.3544 - learning_rate: 5.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22079bc9e50>"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "             ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)]\n",
    "\n",
    "model_cnn_lstm = build_cnn_lstm()\n",
    "model_cnn_lstm.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "ab7412a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LTSM-CNN Train Accuracy: 0.9627049565315247\n",
      "LTSM-CNN Test Accuracy: 0.9019463658332825\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = model_cnn_lstm.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"LTSM-CNN Train Accuracy:\", train_accuracy)\n",
    "\n",
    "test_loss, test_accuracy = model_cnn_lstm.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"LTSM-CNN Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "77f5f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru():\n",
    "    model = Sequential([\n",
    "        Embedding(MAX_FEATURES, 128),\n",
    "        Bidirectional(GRU(128)),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "5d822c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 314ms/step - accuracy: 0.7396 - loss: 0.6458 - val_accuracy: 0.8673 - val_loss: 0.3162 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 307ms/step - accuracy: 0.8878 - loss: 0.2749 - val_accuracy: 0.8731 - val_loss: 0.3058 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 302ms/step - accuracy: 0.9225 - loss: 0.1968 - val_accuracy: 0.9040 - val_loss: 0.2438 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 303ms/step - accuracy: 0.9438 - loss: 0.1455 - val_accuracy: 0.9032 - val_loss: 0.2433 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 304ms/step - accuracy: 0.9540 - loss: 0.1227 - val_accuracy: 0.8881 - val_loss: 0.3066 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 307ms/step - accuracy: 0.9604 - loss: 0.1055 - val_accuracy: 0.9036 - val_loss: 0.2872 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 304ms/step - accuracy: 0.9758 - loss: 0.0706 - val_accuracy: 0.9074 - val_loss: 0.3504 - learning_rate: 5.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x220703e6010>"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gru = build_gru()\n",
    "model_gru.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "6283baea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU Train Accuracy: 0.9643676280975342\n",
      "GRU Test Accuracy: 0.9031503200531006\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = model_gru.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"GRU Train Accuracy:\", train_accuracy)\n",
    "\n",
    "test_loss, test_accuracy = model_gru.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"GRU Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "4b06294e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 32ms/step\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 26ms/step\n"
     ]
    }
   ],
   "source": [
    "probs_lstm = model_lstm.predict(X_test)\n",
    "probs_cnn_lstm = model_cnn_lstm.predict(X_test)\n",
    "probs_gru = model_gru.predict(X_test)\n",
    "\n",
    "avg_probs = (probs_lstm + probs_cnn_lstm + probs_gru) / 3.0\n",
    "\n",
    "ensemble_preds = np.argmax(avg_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "7fc523de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy: 0.9227476422981741\n"
     ]
    }
   ],
   "source": [
    "ensemble_accuracy = accuracy_score(y_test, ensemble_preds)\n",
    "print(\"Ensemble Accuracy:\", ensemble_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "92a88564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model_lstm.save(\"model_lstm.h5\")\n",
    "model_cnn_lstm.save(\"model_cnn_lstm.h5\")\n",
    "model_gru.save(\"model_gru.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ae2713",
   "metadata": {},
   "source": [
    "# inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87090378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_probs(text, model, tokenizer, max_len):\n",
    "    # Tokenize and pad\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(seq, maxlen=max_len)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    probs = model.predict(padded, verbose=0)[0] \n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc4657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_ensemble_sentiment(text, models, tokenizer, max_len):\n",
    "    \n",
    "    all_probs = [predict_sentiment_probs(text, model, tokenizer, max_len) for model in models]\n",
    "    \n",
    "    avg_probs = np.mean(all_probs, axis=0)\n",
    "    \n",
    "    final_label = np.argmax(avg_probs)\n",
    "\n",
    "    return final_label, avg_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a761e819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Prediksi Sentimen: Netral\n",
      "Probabilities: [0.01215679 0.95123476 0.03660843]\n"
     ]
    }
   ],
   "source": [
    "text_input = \"aplikasinya biasa aja\"\n",
    "\n",
    "models = [model_lstm, model_cnn_lstm, model_gru]  \n",
    "label_index = {0: 'Negatif', 1: 'Netral', 2: 'Positif'}\n",
    "\n",
    "pred_label, probs = predict_ensemble_sentiment(text_input, models, tokenizer, MAX_LEN)\n",
    "print(\"Ensemble Prediksi Sentimen:\", label_index[pred_label])\n",
    "print(\"Probabilities:\", probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37c1f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Prediksi Sentimen: Positif\n",
      "Probabilities: [2.9339817e-06 1.1116184e-02 9.8888087e-01]\n"
     ]
    }
   ],
   "source": [
    "text_input = \"aplikasinya sangat bagus sekali\"\n",
    "\n",
    "models = [model_lstm, model_cnn_lstm, model_gru]  \n",
    "label_index = {0: 'Negatif', 1: 'Netral', 2: 'Positif'}\n",
    "\n",
    "pred_label, probs = predict_ensemble_sentiment(text_input, models, tokenizer, MAX_LEN)\n",
    "print(\"Ensemble Prediksi Sentimen:\", label_index[pred_label])\n",
    "print(\"Probabilities:\", probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77992039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Prediksi Sentimen: Negatif\n",
      "Probabilities: [9.9078912e-01 9.2096915e-03 1.1557902e-06]\n"
     ]
    }
   ],
   "source": [
    "text_input = \"aplikasinya jelek banget\"\n",
    "\n",
    "models = [model_lstm, model_cnn_lstm, model_gru]  \n",
    "label_index = {0: 'Negatif', 1: 'Netral', 2: 'Positif'}\n",
    "\n",
    "pred_label, probs = predict_ensemble_sentiment(text_input, models, tokenizer, MAX_LEN)\n",
    "print(\"Ensemble Prediksi Sentimen:\", label_index[pred_label])\n",
    "print(\"Probabilities:\", probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "ed1cddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
