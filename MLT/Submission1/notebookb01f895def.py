# -*- coding: utf-8 -*-
"""notebookb01f895def

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/muhammadrizkiputra/notebookb01f895def.b506c7fd-b82a-4943-aff6-9101f74f3cef.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250523/auto/storage/goog4_request%26X-Goog-Date%3D20250523T191740Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D44c7f33a4b94cde1d3fc3298ad9ee4af958c0eeedbb7fbe2f7218c0fe27d2bfce69cef6038316c8a6880e2cc69ee3cf100d455c6fab01b92826e47e69fa0434ea2b15a5b6ae465bb0401f6a123f0c44d680218ff6d2b0b0607cbf1158085c1e7b7f98281d084b087e400d3bb4469b8556a0a9074b861a7e832ccd736654f70252b54d06a6a1b501265d2cd115a516912799a9ad452441ad504da3e0e358c9f2bab2ca2cca51775f7e409261f5d60308f580e4d7b7b1ee7422683bb4c316eac27b7682ccaa4c9c052c718303f6321d9ed08e11a6477a99dd871c7163d2938d93dbe6feb01c300adbe48cf8c7b667245c10623b3607c0ce64f46e7470f0b1ebb71
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
fedesoriano_heart_failure_prediction_path = kagglehub.dataset_download('fedesoriano/heart-failure-prediction')

print('Data source import complete.')

"""# Laporan Proyek Machine Learning - Muhammad Rizki Putra
## Domain Project
---

#### Judul
Penerapan Algoritma Klasifikasi Machine Learning dalam Memprediksi Risiko Penyakit Jantung

#### Latar Belakang
Penyakit jantung merupakan penyebab utama kematian di seluruh dunia. Berdasarkan data dari World Health Organization (WHO), penyakit kardiovaskular menyebabkan sekitar 17,9 juta kematian setiap tahun, atau sekitar 31% dari total kematian global. Kondisi ini mencerminkan urgensi dalam melakukan deteksi dini untuk mencegah komplikasi yang lebih serius dan meningkatkan harapan hidup pasien.

Seiring dengan kemajuan teknologi, metode pembelajaran mesin (machine learning) telah banyak digunakan untuk mengembangkan model prediktif dalam bidang kesehatan, termasuk untuk memprediksi risiko penyakit jantung. Model ini mampu menganalisis data klinis pasien dan mengenali pola yang tidak terlihat oleh manusia, sehingga dapat digunakan untuk memperkirakan kemungkinan seseorang menderita penyakit jantung.

## Business Understanding
---

### Problem Statements
- Penyakit jantung merupakan salah satu penyebab utama kematian di seluruh dunia, termasuk di Indonesia.
- Proses diagnosis sering kali kompleks, memerlukan waktu, serta bergantung pada interpretasi manual, yang dapat menyebabkan keterlambatan dalam penanganan dan menurunkan peluang pemulihan pasien.

### Goals
- Membangun model machine learning yang mampu memprediksi status penyakit jantung berdasarkan data klinis pasien.
- Mengevaluasi dan membandingkan kinerja beberapa algoritma klasifikasi seperti Logistic Regression, Decision Tree, Random Forest, Gaussian Naive Bayes, Support Vector Machine (SVM), K-Nearest Neighbors (KNN), XGBoost, dan Voting Classifier.
- Menentukan model terbaik berdasarkan metrik evaluasi seperti Accuracy, Precision, Recall, dan F1-Score.
- Memilih model yang tidak hanya memiliki performa tinggi tetapi juga interpretabilitas yang baik untuk mendukung pengambilan keputusan klinis.
  
### Solution statements
Untuk mencapai tujuan-tujuan yang telah ditetapkan, proyek ini akan mengimplementasikan langkah-langkah strategis berikut:

1. Pengembangan Model Prediktif Berbasis Klasifikasi:
Akan dikembangkan sebuah sistem prediksi penyakit jantung dengan menerapkan berbagai algoritma machine learning untuk klasifikasi. Proses ini akan dimulai dengan pengumpulan dan persiapan data klinis pasien yang relevan. Data tersebut kemudian akan melalui tahap pra-pemrosesan (meliputi pembersihan data, penanganan nilai yang hilang jika ada, dan transformasi fitur jika diperlukan) untuk memastikan kualitas dan kesesuaiannya untuk pemodelan.

2. Implementasi dan Pelatihan Ragam Algoritma Klasifikasi:
Serangkaian algoritma klasifikasi yang telah diidentifikasi dalam goals (yaitu Logistic Regression, Decision Tree, Random Forest, Gaussian Naive Bayes, Support Vector Machine (SVM), K-Nearest Neighbors (KNN), XGBoost, dan Voting Classifier) akan diimplementasikan dan dilatih menggunakan dataset yang telah dipersiapkan. Setiap algoritma akan dikonfigurasi dan dioptimalkan (misalnya melalui hyperparameter tuning) untuk mendapatkan performa terbaiknya.

3. Evaluasi Kinerja Model Secara Komprehensif:
Kinerja dari setiap model yang telah dilatih akan diukur dan dibandingkan secara kuantitatif. Pengukuran ini akan menggunakan metrik evaluasi standar untuk tugas klasifikasi, yaitu Accuracy, Precision, Recall, dan F1-Score. Hasil dari metrik-metrik ini akan menjadi dasar objektif untuk menentukan model mana yang paling efektif dalam memprediksi penyakit jantung.

4. Pemilihan Model Optimal dengan Pertimbangan Interpretabilitas:
Berdasarkan hasil evaluasi kinerja, akan dilakukan analisis untuk menentukan model terbaik. Selain metrik kuantitatif, aspek interpretabilitas model juga akan menjadi kriteria penting. Tujuannya adalah untuk memilih model yang tidak hanya akurat dan handal dalam prediksi, tetapi juga dapat memberikan pemahaman atau wawasan yang jelas mengenai faktor-faktor yang mempengaruhi prediksi, sehingga dapat mendukung proses pengambilan keputusan klinis secara lebih efektif.

## Data Undertanding
---

Sumber dataset : https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction

Dataset yang digunakan dalam proyek ini adalah Heart Failure Prediction Dataset, yang dapat diunduh secara publik melalui platform Kaggle. Dataset ini bertujuan untuk membantu dalam prediksi penyakit jantung berdasarkan atribut klinis pasien.

Dataset terdiri dari 918 baris data (masing-masing mewakili seorang pasien) dan 12 kolom fitur yang terdiri dari kombinasi data numerik dan kategorikal. Dataset tidak memiliki nilai kosong (missing values) secara eksplisit, namun terdapat beberapa nilai nol pada fitur tertentu (misalnya Cholesterol) yang dicurigai sebagai representasi dari data hilang.

## Visualization & Explanatory Analysis
---

Tahap visualisasi data dilakukan untuk menyajikan hasil analisis dalam bentuk grafik dan diagram yang lebih mudah dipahami. Teknik visualisasi seperti histogram, scatter plot, heatmap, dan box plot digunakan untuk melihat distribusi data, hubungan antar variabel, serta pola yang mungkin muncul dalam dataset.

Selain itu, dilakukan explanatory analysis untuk menjelaskan temuan dari proses eksplorasi dan pemodelan. Interpretasi visualisasi membantu dalam mengidentifikasi tren signifikan, anomali, serta wawasan penting yang dapat digunakan untuk pengambilan keputusan.

Dengan pendekatan ini, hasil analisis menjadi lebih informatif dan dapat disajikan dengan cara yang jelas serta mudah dipahami.

### Import Library
Melakukan import library untuk mendukung analisis data dalam laporan ini. Pustaka seperti Pandas, NumPy, dan Matplotlib digunakan untuk manipulasi data, analisis statistik, serta visualisasi hasil.
"""

# Manipulasi Data
import numpy as np
import pandas as pd

# Visualisasi Data
import matplotlib.pyplot as plt
import seaborn as sns

# Preprocessing
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Model Selection dan Tuning
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Algoritma Klasifikasi
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier

# Evaluasi Model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

"""### Gathering Data
Pada tahap ini, kita membaca dataset yang dibutuhkan, yaitu: heart_csv
"""

source_dir = "/kaggle/input/heart-failure-prediction/heart.csv"
destination_dir = "/kaggle/working/submission"

heart_df = pd.read_csv(source_dir,delimiter=",")
heart_df.head()

"""### Exploratory Data Analysis (EDA)

dilakukan untuk memahami karakteristik data sebelum proses pemodelan. Analisis ini mencakup pemeriksaan distribusi data, deteksi nilai kosong, identifikasi anomali
"""

heart_df.info()

print(f"Duplicate : {heart_df.duplicated().sum()}")
heart_df.isna().sum()

heart_df.describe()

"""- Count adalah jumlah sampel pada data.
- Mean adalah nilai rata-rata.
- Std adalah standar deviasi.
- Min yaitu nilai minimum.
- 25% adalah kuartil pertama.
- 50% adalah kuartil kedua.
- 75% adalah kuartil ketiga.
- Max adalah nilai maksimum.

### Data Visualization

Visualisasi data dilakukan untuk memahami pola dan hubungan antar variabel dalam dataset. Dengan menggunakan grafik seperti histogram, scatter plot, dan heatmap, data dapat ditampilkan dengan lebih intuitif sehingga memudahkan analisis. Teknik visualisasi yang tepat dapat membantu dalam mengidentifikasi tren, distribusi, serta anomali yang mungkin terjadi dalam data.
"""

sns.countplot(x='HeartDisease', data=heart_df)
plt.title('Distribusi Pasien dengan Penyakit Jantung')
plt.xlabel('Heart Disease (0 = Tidak, 1 = Ya)')
plt.ylabel('Jumlah Pasien')
plt.show()

sns.histplot(data=heart_df, x='Age', kde=True, bins=20)
plt.title('Distribusi Usia Pasien')
plt.xlabel('Umur')
plt.ylabel('Jumlah')
plt.show()

sns.boxplot(x='HeartDisease', y='Age', data=heart_df)
plt.title('Usia Pasien Berdasarkan Status Penyakit Jantung')
plt.xlabel('Heart Disease')
plt.ylabel('Umur')
plt.show()

correlation = heart_df.corr(numeric_only=True)
plt.figure(figsize=(10, 6))
sns.heatmap(correlation, annot=True, cmap='coolwarm')
plt.title('Matriks Korelasi Antar Fitur Numerik')
plt.show()

sns.countplot(x='ChestPainType', hue='HeartDisease', data=heart_df)
plt.title('Tipe Nyeri Dada vs Penyakit Jantung')
plt.xlabel('Tipe Nyeri Dada')
plt.ylabel('Jumlah Pasien')
plt.show()

sns.boxplot(x='ExerciseAngina', y='Oldpeak', data=heart_df)
plt.title('Oldpeak berdasarkan Exercise Angina')
plt.show()

"""### Insight

1. Dataset Overview
- Dataset berhasil dimuat ke dalam `heart_df` dari file `heart.csv`.
- **Jumlah data:** 918 entri pasien  
- **Jumlah fitur:** 12 kolom — terdiri dari:
  - **7 fitur numerik**: `Age`, `RestingBP`, `Cholesterol`, `FastingBS`, `MaxHR`, `Oldpeak`, `HeartDisease`
  - **5 fitur kategorikal**: `Sex`, `ChestPainType`, `RestingECG`, `ExerciseAngina`, `ST_Slope`
- **Data sudah bersih**, namun ditemukan indikasi nilai nol yang seharusnya hilang (false zero).

2. Statistik Deskriptif Fitur Numerik

| Fitur           | Mean   | Median | Modus | Std Dev | Insight                                                                 |
|------------------|--------|--------|--------|---------|-------------------------------------------------------------------------|
| **Age**          | 53.51  | 54.0   | 54.0   | 9.43    | Distribusi usia relatif normal. Sebagian besar pasien berusia 40–65.   |
| **RestingBP**    | 132.4  | 130.0  | 120.0  | 18.51   | Tekanan darah istirahat normal, dengan variasi yang cukup besar.        |
| **Cholesterol**  | 198.8  | 223.0  | 0.0    | 109.38  | Modus 0 tidak wajar → kemungkinan **nilai hilang terselubung** (false zero). |
| **FastingBS**    | 0.23   | 0.0    | 0.0    | 0.42    | Mayoritas pasien tidak mengalami kadar gula puasa tinggi.              |
| **MaxHR**        | 136.8  | 138.0  | 150.0  | 25.46   | Detak jantung maksimum cukup tinggi dan menyebar luas.                 |
| **Oldpeak**      | 0.89   | 0.6    | 0.0    | 1.07    | Banyak pasien tidak menunjukkan depresi segmen ST.                     |
| **HeartDisease** | 0.55   | 1.0    | 1.0    | 0.50    | Sekitar 55% pasien didiagnosis penyakit jantung.                       |

3. Insight Tambahan Berdasarkan Visualisasi

Distribusi Pasien dengan Penyakit Jantung
- Mayoritas pasien (sekitar 55%) menderita penyakit jantung (`HeartDisease = 1`).
- Terdapat ketidakseimbangan label, namun masih dalam batas aman tanpa oversampling.

Distribusi Usia Pasien
- Sebagian besar pasien berusia 40–65 tahun.
- Distribusi usia menyerupai distribusi normal.
- Pasien yang lebih tua memiliki risiko lebih tinggi terhadap penyakit jantung.

Usia vs Status Penyakit Jantung
- Median usia pasien dengan penyakit jantung lebih tinggi dari yang tidak.
- Terdapat outlier: pasien muda juga bisa mengalami penyakit jantung → usia bukan satu-satunya faktor risiko.

Korelasi Antar Fitur Numerik
- Korelasi paling kuat terhadap `HeartDisease`:
  - `Oldpeak` (positif): makin tinggi → makin tinggi risiko penyakit jantung.
  - `MaxHR` (negatif): makin tinggi → makin rendah risiko penyakit jantung.
- Fitur lain memiliki korelasi rendah → fitur tidak redundant.

Tipe Nyeri Dada vs Penyakit Jantung
- Tipe `ASY` (Asymptomatic) dominan pada pasien dengan penyakit jantung.
- Tipe `ATA` dan `NAP` lebih umum pada pasien tanpa penyakit jantung.
- Tipe nyeri dada merupakan fitur kategorikal yang **informatif dan penting** untuk prediksi.

Oldpeak Berdasarkan Exercise Angina
- Nilai `Oldpeak` lebih tinggi pada pasien dengan `ExerciseAngina = Y`.
- Menunjukkan hubungan kuat antara `Oldpeak` dan `ExerciseAngina` sebagai indikator penyakit jantung.

4. Rencana Transformasi Data
- Beberapa fitur kategorikal yang akan dikonversi ke bentuk numerik:
  - `Sex`
  - `ChestPainType`
  - `RestingECG`
  - `ExerciseAngina`
  - `ST_Slope`
- Penanganan nilai anomali:
  - Ganti nilai `0` pada kolom `Cholesterol` dengan `NaN` untuk mencerminkan missing value yang sebenarnya.

## Data Preparation
---

Data Preparation adalah tahap penting sebelum melakukan pemodelan machine learning. Tujuan dari langkah ini adalah memastikan data yang digunakan memiliki kualitas yang baik, bebas dari ketidaksempurnaan, serta siap untuk digunakan dalam algoritma pemodelan.

Dalam laporan ini, proses data preparation dilakukan dengan langkah-langkah berikut:

1. Mengganti Nilai 0 yang Tidak Valid pada Kolom Cholesterol – Mengatasi nilai yang tidak logis dengan metode imputasi atau menggantinya sesuai distribusi data.
2. Encoding Variabel Kategorikal – Mengubah data kategorikal menjadi format numerik yang bisa dipahami oleh model, seperti One-Hot Encoding atau Label Encoding.
3. Membagi Data Menjadi Train dan Test Set – Memisahkan data untuk proses pelatihan dan evaluasi model guna memastikan performa yang akurat dan menghindari overfitting.
4. Normalisasi Fitur Numerik (Feature Scaling) – Menyelaraskan skala fitur agar model dapat beroperasi dengan baik, menggunakan teknik seperti Min-Max Scaling atau Standardization.

### Mengganti Nilai 0 yang Tidak Valid pada Kolom Cholesterol
"""

heart_df['Cholesterol'] = heart_df['Cholesterol'].replace(0, np.nan)
heart_df['Cholesterol'] = heart_df['Cholesterol'].fillna(heart_df['Cholesterol'].median())

"""### Encoding"""

label_enc_cols = ['Sex', 'ExerciseAngina']
onehot_enc_cols = ['ChestPainType', 'RestingECG', 'ST_Slope']

# Label encoding
for col in label_enc_cols:
    le = LabelEncoder()
    heart_df[col] = le.fit_transform(heart_df[col])

# One-hot encoding
heart_df = pd.get_dummies(heart_df, columns=onehot_enc_cols)

"""### Membagi Data Menjadi Train dan Test Set"""

X = heart_df.drop('HeartDisease', axis=1)
y = heart_df['HeartDisease']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""### Normalisasi Fitur Numerik"""

num_features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']
scaler = StandardScaler()

X_train_scaled = X_train.copy()
X_train_scaled[num_features] = scaler.fit_transform(X_train[num_features])

X_test_scaled = X_test.copy()
X_test_scaled[num_features] = scaler.transform(X_test[num_features])

"""## Modeling
---

Untuk menyelesaikan permasalahan prediksi risiko penyakit jantung, digunakan pendekatan supervised learning dengan tugas klasifikasi biner (risiko penyakit jantung: ada atau tidak). Proses pemodelan dilakukan melalui tahapan sebagai berikut:

Modeling adalah tahap di mana data yang telah dipersiapkan digunakan untuk membangun dan melatih model machine learning. Tujuan utama dari proses ini adalah mendapatkan model yang dapat melakukan prediksi atau klasifikasi dengan tingkat akurasi tinggi.

Dalam laporan ini, proses modeling dilakukan dengan tahapan berikut:

1. Memilih Algoritma – Menentukan metode machine learning yang sesuai berdasarkan karakteristik data dan tujuan analisis.
2. Melatih Model – Menggunakan data training untuk membangun model dan menyesuaikan parameter agar mencapai performa optimal.

### Memilih Algoritma
"""

# Logistic Regression
lr = LogisticRegression(max_iter=1000, random_state=42)

# Decision Tree
dt = DecisionTreeClassifier(random_state=42)

# Random Forest
rf = RandomForestClassifier(random_state=42)

# KNN
knn = KNeighborsClassifier(n_neighbors=5)  # Default k=5

# SVM
svm = SVC(kernel='linear', random_state=42)  # Gunakan kernel linear untuk interpretabilitas

"""### Melatih Model"""

lr.fit(X_train_scaled, y_train)

# Prediksi
y_pred_train_lr = lr.predict(X_train_scaled)
y_pred_test_lr = lr.predict(X_test_scaled)

# Akurasi
accuracy_train_lr = accuracy_score(y_train, y_pred_train_lr)
accuracy_test_lr = accuracy_score(y_test, y_pred_test_lr)

print('Logistic Regression - accuracy_train:', accuracy_train_lr)
print('Logistic Regression - accuracy_test:', accuracy_test_lr)

dt.fit(X_train_scaled, y_train)

# Prediksi
y_pred_train_dt = dt.predict(X_train_scaled)
y_pred_test_dt = dt.predict(X_test_scaled)

# Akurasi
accuracy_train_dt = accuracy_score(y_train, y_pred_train_dt)
accuracy_test_dt = accuracy_score(y_test, y_pred_test_dt)

print('Decision Tree - accuracy_train:', accuracy_train_dt)
print('Decision Tree - accuracy_test:', accuracy_test_dt)

rf.fit(X_train_scaled, y_train)

# Prediksi
y_pred_train_rf = rf.predict(X_train_scaled)
y_pred_test_rf = rf.predict(X_test_scaled)

# Akurasi
accuracy_train_rf = accuracy_score(y_train, y_pred_train_rf)
accuracy_test_rf = accuracy_score(y_test, y_pred_test_rf)

print('Random Forest - accuracy_train:', accuracy_train_rf)
print('Random Forest - accuracy_test:', accuracy_test_rf)

nb = GaussianNB().fit(X_train_scaled, y_train)
nb.fit(X_train_scaled, y_train)

# Prediksi
y_pred_train_nb = nb.predict(X_train_scaled)
y_pred_test_nb = nb.predict(X_test_scaled)

# Akurasi
accuracy_train_nb = accuracy_score(y_train, y_pred_train_nb)
accuracy_test_nb = accuracy_score(y_test, y_pred_test_nb)

print('GaussianNB - accuracy_train:', accuracy_train_nb)
print('GaussianNB - accuracy_test:', accuracy_test_nb)

knn.fit(X_train_scaled, y_train)

# Prediksi
y_pred_train_knn = knn.predict(X_train_scaled)
y_pred_test_knn = knn.predict(X_test_scaled)

# Akurasi
accuracy_train_knn = accuracy_score(y_train, y_pred_train_knn)
accuracy_test_knn = accuracy_score(y_test, y_pred_test_knn)

print('KNN - accuracy_train:', accuracy_train_knn)
print('KNN - accuracy_test:', accuracy_test_knn)

svm.fit(X_train_scaled, y_train)

# Prediksi
y_pred_train_svm = svm.predict(X_train_scaled)
y_pred_test_svm = svm.predict(X_test_scaled)

# Akurasi
accuracy_train_svm = accuracy_score(y_train, y_pred_train_svm)
accuracy_test_svm = accuracy_score(y_test, y_pred_test_svm)

print('SVM - accuracy_train:', accuracy_train_svm)
print('SVM - accuracy_test:', accuracy_test_svm)

"""## Evaluation

Tahap evaluasi dilakukan untuk menilai performa model yang telah dibangun. Metrik seperti akurasi, precision, recall, dan F1-score digunakan untuk mengukur seberapa baik model dalam membuat prediksi. Selain itu, dilakukan analisis terhadap error dan bias yang mungkin terjadi guna meningkatkan kualitas model serta memastikan hasil yang lebih akurat. Jika diperlukan, teknik optimasi seperti tuning hyperparameter atau pemilihan fitur dapat diterapkan untuk meningkatkan performa model.

Untuk mengevaluasi performa model, digunakan empat metrik utama yaitu:

- **Accuracy**: proporsi prediksi yang benar dari seluruh prediksi.
- **Precision**: proporsi prediksi positif yang benar dari seluruh prediksi positif.
- **Recall**: proporsi data positif yang berhasil teridentifikasi.
- **F1-Score**: rata-rata harmonik dari precision dan recall.
"""

def evaluate_model(model, X_test_scaled, y_test, model_name="Model"):
    y_pred = model.predict(X_test_scaled)
    cm = confusion_matrix(y_test, y_pred)

    results = {
        'Model': model_name,
        'Confusion Matrix': cm,
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred, average='weighted'),
        'Recall': recall_score(y_test, y_pred, average='weighted'),
        'F1-Score': f1_score(y_test, y_pred, average='weighted'),
        'Classification Report': classification_report(y_test, y_pred, output_dict=True)
    }

    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title(f"Confusion Matrix - {model_name}")
    plt.show()

    return results

models = {
    'K-Nearest Neighbors (KNN)': knn,
    'Support Vector Machine (SVM)': svm,
    'Decision Tree': dt,
    'Random Forest': rf,
    'Logistic Regression': lr,
    'Naive Bayes': nb
}

results = {name: evaluate_model(model, X_test_scaled, y_test, name) for name, model in models.items()}

summary_df = pd.DataFrame([
    {
        'Model': name,
        'Accuracy': metrics['Accuracy'],
        'Precision': metrics['Precision'],
        'Recall': metrics['Recall'],
        'F1-Score': metrics['F1-Score']
    }
    for name, metrics in results.items()
])

print(summary_df)

"""### Tuning Model

Selanjutnya dilakukan proses roses tuning hyperparameter untuk model Decision Tree menggunakan teknik RandomizedSearchCV yang bertujuan untuk mencari kombinasi hyperparameter terbaik untuk model Decision Tree guna meningkatkan performa prediksi penyakit jantung.
"""

param_dist = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 10, 20, 30, 40, 50, None],
    'min_samples_split': np.arange(2, 20, 1),
    'min_samples_leaf': np.arange(1, 20, 1),
    'max_features': ['sqrt', 'log2', None]
}

random_search = RandomizedSearchCV(DecisionTreeClassifier(), param_distributions=param_dist,
                                   n_iter=100, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)
random_search.fit(X_train, y_train)

print("Best Parameters:", random_search.best_params_)
print("Best Accuracy:", random_search.best_score_)

best_tree = random_search.best_estimator_

"""### Modeling dengan Voting Classifier

Model ini menggunakan pendekatan Voting Classifier untuk menggabungkan beberapa algoritma machine learning guna meningkatkan akurasi prediksi. Model yang digunakan mencakup Decision Tree, Random Forest, Support Vector Machine (SVM), Gaussian Naïve Bayes, Logistic Regression, K-Nearest Neighbors (KNN), dan XGBoost. Dengan teknik soft voting, setiap model memberikan probabilitas prediksi yang kemudian dikombinasikan berdasarkan bobot tertentu. Model ini dilatih menggunakan data training dan diuji pada data test, menghasilkan metrik akurasi untuk masing-masing model serta keseluruhan Voting Classifier. Tujuan dari pendekatan ini adalah meningkatkan kinerja model dengan memanfaatkan keunggulan dari masing-masing metode.
"""

tree = DecisionTreeClassifier(**random_search.best_params_)
rf = RandomForestClassifier(n_estimators=200)
svm = SVC(probability=True)
nb = GaussianNB()
lr = LogisticRegression(max_iter=1000)
knn = KNeighborsClassifier(n_neighbors=5)
xgb = XGBClassifier(n_estimators=200, learning_rate=0.1)

voting_clf = VotingClassifier(
    estimators=[
        ('dt', tree),
        ('rf', rf),
        ('svm', svm),
        ('nb', nb),
        ('lr', lr),
        ('knn', knn),
        ('xgb', xgb)
    ],
    voting='soft',
    weights=[1,2,1,1,2,2,1]
)

voting_clf.fit(X_train_scaled, y_train)

model_performance_data = []

# Evaluasi Voting Classifier
y_pred_voting_clf = voting_clf.predict(X_test_scaled)
accuracy_voting_clf = accuracy_score(y_test, y_pred_voting_clf)
precision_voting_clf = precision_score(y_test, y_pred_voting_clf, average='weighted', zero_division=0)
recall_voting_clf = recall_score(y_test, y_pred_voting_clf, average='weighted', zero_division=0)
f1_voting_clf = f1_score(y_test, y_pred_voting_clf, average='weighted', zero_division=0)

model_performance_data.append({
    'Model': 'Voting Classifier',
    'Accuracy': accuracy_voting_clf,
    'Precision': precision_voting_clf,
    'Recall': recall_voting_clf,
    'F1-Score': f1_voting_clf
})

print("--- Voting Classifier Metrics ---")
print(f"Accuracy: {accuracy_voting_clf:.4f}")
print(f"Precision: {precision_voting_clf:.4f}")
print(f"Recall: {recall_voting_clf:.4f}")
print(f"F1-Score: {f1_voting_clf:.4f}")
print(classification_report(y_test, y_pred_voting_clf, zero_division=0))
print("-" * 30)


# Evaluasi Model Individual dalam Voting Classifier
for name, model in voting_clf.named_estimators_.items():
    # Model sudah di-fit sebagai bagian dari voting_clf.fit()
    y_pred_individual = model.predict(X_test_scaled)

    accuracy_individual = accuracy_score(y_test, y_pred_individual)
    precision_individual = precision_score(y_test, y_pred_individual, average='weighted', zero_division=0)
    recall_individual = recall_score(y_test, y_pred_individual, average='weighted', zero_division=0)
    f1_individual = f1_score(y_test, y_pred_individual, average='weighted', zero_division=0)

    model_performance_data.append({
        'Model': name.upper(),
        'Accuracy': accuracy_individual,
        'Precision': precision_individual,
        'Recall': recall_individual,
        'F1-Score': f1_individual
    })

    print(f"--- {name.upper()} Metrics ---")
    print(f"Accuracy: {accuracy_individual:.4f}")
    print(f"Precision: {precision_individual:.4f}")
    print(f"Recall: {recall_individual:.4f}")
    print(f"F1-Score: {f1_individual:.4f}")
    print(classification_report(y_test, y_pred_individual, zero_division=0))
    print("-" * 30)

# Membuat DataFrame dari list dictionary
summary_df_voting = pd.DataFrame(model_performance_data)

# Menampilkan DataFrame
print("\n--- Summary DataFrame ---")
print(summary_df_voting)

"""## Hasil Akhir dan Pemilihan Model Terbaik

### Metrik Evaluasi
Untuk mengevaluasi performa model-model yang telah dilatih, digunakan empat metrik utama dalam klasifikasi:
- **Accuracy**: Proporsi prediksi yang benar dari keseluruhan prediksi. Ini mengukur seberapa sering model membuat prediksi yang benar secara keseluruhan.
- **Precision**: Proporsi prediksi positif yang benar dari seluruh observasi yang diprediksi positif. Metrik ini penting ketika biaya *false positive* tinggi.
- **Recall** (Sensitivitas): Proporsi observasi positif aktual yang berhasil diidentifikasi dengan benar oleh model. Metrik ini krusial ketika biaya *false negative* tinggi.
- **F1-Score**: Rata-rata harmonik dari *precision* dan *recall*. Metrik ini memberikan keseimbangan antara *precision* dan *recall*, berguna terutama jika ada ketidakseimbangan kelas.

### Perbandingan Kinerja Model
Berikut adalah tabel ringkasan hasil evaluasi metrik untuk setiap model yang diuji pada data tes:

| Model                         | Accuracy | Precision | Recall   | F1-Score |
|-------------------------------|----------|-----------|----------|----------|
| K-Nearest Neighbors (KNN)     | 0.858696 | 0.860400  | 0.858696 | 0.859139 |
| Support Vector Machine (SVM)  | 0.858696 | 0.861753  | 0.858696 | 0.859307 |
| Decision Tree (DT)            | 0.831522 | 0.832792  | 0.831522 | 0.831934 |
| **Random Forest (RF)** | **0.880435** | **0.882024** | **0.880435** | **0.880810** |
| Logistic Regression (LR)      | 0.864130 | 0.867953  | 0.864130 | 0.864787 |
| Naive Bayes (NB)              | 0.836957 | 0.848806  | 0.836957 | 0.838043 |
| XGBoost (XGB)                 | 0.842391 | 0.846359  | 0.842391 | 0.843153 |
| Voting Classifier             | 0.869565 | 0.871212  | 0.869565 | 0.869974 |

### Pemilihan Model Terbaik

Berdasarkan semua metrik evaluasi pada data uji, **Random Forest (RF)** menunjukkan performa tertinggi secara konsisten, dengan **Accuracy 0.8804, Precision 0.8820, Recall 0.8804, dan F1-Score 0.8808**. Model ini mengungguli model-model individual lainnya serta *Voting Classifier* (Accuracy 0.8696, F1-Score 0.8700).

Meskipun *Voting Classifier* bertujuan untuk meningkatkan stabilitas dan performa dengan menggabungkan beberapa model, dan dalam kasus ini menunjukkan peningkatan dibandingkan beberapa model individual, Random Forest sebagai model tunggal (yang juga merupakan *ensemble* dari *decision tree*) sudah memberikan hasil yang lebih optimal di semua metrik utama.

Jika interpretabilitas menjadi faktor krusial, *Logistic Regression* (Accuracy 0.8641, F1-Score 0.8648) bisa menjadi alternatif yang baik karena kemudahan dalam memahami logika pengambilan keputusannya. Namun, jika fokus utama adalah pada performa prediktif tertinggi di berbagai metrik, **Random Forest** adalah model yang dipilih sebagai solusi terbaik untuk prediksi penyakit jantung dalam penelitian ini.

### Analisis Kelebihan dan Kekurangan Model Terpilih (Random Forest)

* **Kelebihan Random Forest**:
    * Umumnya memiliki akurasi yang tinggi dan robust terhadap *overfitting* dibandingkan *decision tree* tunggal.
    * Mampu menangani data dengan fitur dalam jumlah besar dan menangkap interaksi non-linier.
    * Dapat memberikan estimasi pentingnya fitur (*feature importance*).

* **Kekurangan Random Forest**:
    * Lebih bersifat *black box* dibandingkan *decision tree* tunggal atau regresi logistik, sehingga interpretasi detail tentang bagaimana prediksi dibuat bisa lebih sulit.
    * Membutuhkan sumber daya komputasi yang lebih besar dan waktu pelatihan yang lebih lama, terutama dengan jumlah pohon (`n_estimators`) yang banyak.

### Ringkasan *Insight* Akhir

Proyek ini berhasil mengembangkan dan mengevaluasi berbagai model *machine learning* untuk prediksi penyakit jantung. Dengan menggunakan dataset klinis yang terdiri dari 918 pasien, dan setelah melalui tahapan pra-pemrosesan serta *feature engineering*, model **Random Forest** muncul sebagai model dengan performa terbaik di semua metrik evaluasi utama (Accuracy 0.8804, Precision 0.8820, Recall 0.8804, F1-Score 0.8808) pada data uji. Meskipun pendekatan *ensemble* seperti *Voting Classifier* (Accuracy 0.8696, F1-Score 0.8700) juga menunjukkan kinerja yang sangat baik dan mengungguli sebagian besar model individual, Random Forest secara individual terbukti sedikit lebih unggul. Pemilihan Random Forest sebagai model akhir didasarkan pada performa prediktifnya yang superior, dengan tetap memperhatikan bahwa jika interpretabilitas menjadi prioritas utama, model lain seperti *Logistic Regression* dapat dipertimbangkan. Solusi ini memberikan dasar yang kuat untuk pengembangan sistem pendukung keputusan klinis dalam diagnosis dini penyakit jantung.
"""